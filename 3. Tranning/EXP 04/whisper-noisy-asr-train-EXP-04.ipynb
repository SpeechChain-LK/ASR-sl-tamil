{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12368553,"sourceType":"datasetVersion","datasetId":7798388},{"sourceId":12369551,"sourceType":"datasetVersion","datasetId":7799096},{"sourceId":12385975,"sourceType":"datasetVersion","datasetId":7810055},{"sourceId":12400769,"sourceType":"datasetVersion","datasetId":7820165}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages\n%pip install transformers datasets evaluate jiwer\n%pip install librosa scikit-learn pandas\n%pip install soundfile\n%pip install tensorboard\n%pip install accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:22:08.192456Z","iopub.execute_input":"2025-07-12T04:22:08.193081Z","iopub.status.idle":"2025-07-12T04:23:45.095634Z","shell.execute_reply.started":"2025-07-12T04:22:08.193054Z","shell.execute_reply":"2025-07-12T04:23:45.094290Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nCollecting evaluate\n  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\nCollecting jiwer\n  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, fsspec, jiwer, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.5 fsspec-2025.3.0 jiwer-4.0.0 rapidfuzz-3.13.0\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\nRequirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.2)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\nRequirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\nRequirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2.4.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.3->librosa) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.3->librosa) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.3->librosa) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.3->librosa) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.3->librosa) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.26.4)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->soundfile) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->soundfile) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->soundfile) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->soundfile) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->soundfile) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.72.0rc1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\nRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (25.0)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.12.0->tensorboard) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.12.0->tensorboard) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.12.0->tensorboard) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.12.0->tensorboard) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.12.0->tensorboard) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport librosa\nfrom datasets import Dataset, Audio\nfrom transformers import (\n    WhisperFeatureExtractor, \n    WhisperTokenizer, \n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    EarlyStoppingCallback\n)\nfrom sklearn.model_selection import train_test_split\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport evaluate\nfrom jiwer import wer, cer, mer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:23:45.097957Z","iopub.execute_input":"2025-07-12T04:23:45.098899Z","iopub.status.idle":"2025-07-12T04:24:24.543680Z","shell.execute_reply.started":"2025-07-12T04:23:45.098834Z","shell.execute_reply":"2025-07-12T04:24:24.543107Z"}},"outputs":[{"name":"stderr","text":"2025-07-12 04:24:05.705437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752294246.048875      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752294246.144514      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ================================\n# 1. DATA PREPARATION\n# ================================\n\n# Load the dataset from the TSV file\nfile_path = \"/kaggle/input/noisy-data/noisy_data.tsv\"\ndf = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"ID\", \"Text\"])\n\n# Add full path to audio files\ndf[\"audio\"] = df[\"ID\"].apply(lambda x: f\"/kaggle/input/noisy-audio-data-for-asr/noisy-audio-data-for-ASR/{x}\")\ndf = df.rename(columns={\"Text\": \"sentence\"})\n\n# Remove any rows with missing audio files or text\ndf = df.dropna()\ndf = df[df[\"sentence\"].str.strip() != \"\"]\n\nprint(f\"Dataset size: {len(df)} samples\")\nprint(f\"Sample data:\\n{df.head()}\")\n\n# Train-validation split with stratification if needed\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n\n# Save splits for reference\ntrain_df.to_csv(\"train.csv\", index=False)\nval_df.to_csv(\"val.csv\", index=False)\n\nprint(f\"Training samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(val_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:24:24.544431Z","iopub.execute_input":"2025-07-12T04:24:24.545116Z","iopub.status.idle":"2025-07-12T04:24:24.658739Z","shell.execute_reply.started":"2025-07-12T04:24:24.545086Z","shell.execute_reply":"2025-07-12T04:24:24.658074Z"}},"outputs":[{"name":"stdout","text":"Dataset size: 4284 samples\nSample data:\n                                ID  \\\n0  taf_02345_00348037167_noisy.wav   \n1  taf_07049_00155837462_noisy.wav   \n2  taf_09705_01218130267_noisy.wav   \n3  taf_03219_00712757493_noisy.wav   \n4  taf_00008_01305524612_noisy.wav   \n\n                                            sentence  \\\n0  ஆஸ்த்ரேலியப் பெண்ணுக்கு முப்பத்தி மூன்று ஆண்டு...   \n1            ஸ்ரீரங்கம் கோவிலில் வெடிகுண்டு மிரட்டல்   \n2  உங்களுடைய உணவுக் கட்டுப்பாட்டைச் சொன்னால் மற்ற...   \n3  ரஹானே மட்டும் ஆறுதலாக முப்பத்தி ஆறு ரன்கள் குவ...   \n4               மனோரமாவிற்கு பூபதி என்ற மகன் உள்ளார்   \n\n                                               audio  \n0  /kaggle/input/noisy-audio-data-for-asr/noisy-a...  \n1  /kaggle/input/noisy-audio-data-for-asr/noisy-a...  \n2  /kaggle/input/noisy-audio-data-for-asr/noisy-a...  \n3  /kaggle/input/noisy-audio-data-for-asr/noisy-a...  \n4  /kaggle/input/noisy-audio-data-for-asr/noisy-a...  \nTraining samples: 3427\nValidation samples: 857\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ================================\n# 2. AUDIO PREPROCESSING & AUGMENTATION\n# ================================\n\ndef verify_audio_file(audio_path):\n    \"\"\"Verify if audio file exists and is readable\"\"\"\n    try:\n        audio, sr = librosa.load(audio_path, sr=16000)\n        return len(audio) > 0\n    except:\n        return False\n\ndef add_noise_augmentation(audio_array, noise_factor=0.005):\n    \"\"\"Add Gaussian noise for additional robustness\"\"\"\n    noise = np.random.normal(0, noise_factor, audio_array.shape)\n    return audio_array + noise\n\ndef normalize_audio(audio_array):\n    \"\"\"Normalize audio to prevent clipping\"\"\"\n    max_val = np.max(np.abs(audio_array))\n    if max_val > 0:\n        return audio_array / max_val\n    return audio_array\n\n# Filter out invalid audio files\nvalid_train = []\nvalid_val = []\n\nprint(\"Validating audio files...\")\nfor idx, row in train_df.iterrows():\n    if verify_audio_file(row['audio']):\n        valid_train.append(row)\n    else:\n        print(f\"Invalid audio file: {row['audio']}\")\n\nfor idx, row in val_df.iterrows():\n    if verify_audio_file(row['audio']):\n        valid_val.append(row)\n    else:\n        print(f\"Invalid audio file: {row['audio']}\")\n\ntrain_df = pd.DataFrame(valid_train)\nval_df = pd.DataFrame(valid_val)\n\nprint(f\"Valid training samples: {len(train_df)}\")\nprint(f\"Valid validation samples: {len(val_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:24:24.660234Z","iopub.execute_input":"2025-07-12T04:24:24.660461Z","iopub.status.idle":"2025-07-12T04:25:28.993988Z","shell.execute_reply.started":"2025-07-12T04:24:24.660443Z","shell.execute_reply":"2025-07-12T04:25:28.993158Z"}},"outputs":[{"name":"stdout","text":"Validating audio files...\nValid training samples: 3427\nValid validation samples: 857\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ================================\n# 3. WHISPER PROCESSOR SETUP\n# ================================\n\n# Initialize Whisper components for Tamil\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"ta\", task=\"transcribe\")\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"ta\", task=\"transcribe\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:25:28.994689Z","iopub.execute_input":"2025-07-12T04:25:28.995235Z","iopub.status.idle":"2025-07-12T04:25:30.810990Z","shell.execute_reply.started":"2025-07-12T04:25:28.995215Z","shell.execute_reply":"2025-07-12T04:25:30.810024Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d646b0a5aa7d4d6ebc94cc0cf775fcd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecaf6ccdc42946b4aea59d1f22edaae8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"442b23d8dd124782a63b48a75ea49858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3557af80b684485aa3151d4d3323dd4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89372ab0c39c4f61b15c5ff45e7c1ab1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6c187a21cd43f5983d00b9fa3a721d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4bf1d44b38e45e88d38fcf66d44979e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"982cc08227684a01af77ff39fc9d92fa"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ================================\n# 4. DATASET CREATION WITH NOISE HANDLING\n# ================================\n\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Cast audio with target sampling rate\ntrain_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\nval_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n\ndef prepare_dataset(examples):\n    \"\"\"Prepare dataset with noise-robust preprocessing\"\"\"\n    # Load and process audio\n    audio = examples[\"audio\"]\n    audio_array = audio[\"array\"]\n    \n    # Apply noise-robust preprocessing\n    audio_array = normalize_audio(audio_array)\n    \n    # Optional: Add slight additional noise for robustness (uncomment if needed)\n    # audio_array = add_noise_augmentation(audio_array, noise_factor=0.001)\n    \n    # Ensure audio is not too short or too long\n    min_length = 1000  # ~0.06 seconds at 16kHz\n    max_length = 480000  # ~30 seconds at 16kHz\n    \n    if len(audio_array) < min_length:\n        # Pad short audio\n        audio_array = np.pad(audio_array, (0, min_length - len(audio_array)), 'constant')\n    elif len(audio_array) > max_length:\n        # Truncate long audio\n        audio_array = audio_array[:max_length]\n    \n    # Compute log-Mel input features\n    examples[\"input_features\"] = feature_extractor(\n        audio_array, sampling_rate=16000\n    ).input_features[0]\n    \n    # Clean up\n    del examples[\"audio\"]\n    \n    # Process text\n    sentences = examples[\"sentence\"]\n    \n    # Clean and normalize text\n    if isinstance(sentences, str):\n        sentences = sentences.strip()\n    \n    # Encode target text to label ids\n    examples[\"labels\"] = tokenizer(sentences).input_ids\n    del examples[\"sentence\"]\n    \n    return examples\n\n# Apply preprocessing\nprint(\"Preprocessing training dataset...\")\ntrain_dataset = train_dataset.map(prepare_dataset, num_proc=1)\n\nprint(\"Preprocessing validation dataset...\")\nval_dataset = val_dataset.map(prepare_dataset, num_proc=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:25:30.812073Z","iopub.execute_input":"2025-07-12T04:25:30.812394Z","iopub.status.idle":"2025-07-12T04:26:46.094663Z","shell.execute_reply.started":"2025-07-12T04:25:30.812368Z","shell.execute_reply":"2025-07-12T04:26:46.093916Z"}},"outputs":[{"name":"stdout","text":"Preprocessing training dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3427 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ab16ef818e487ba79bcb1ac0ac1c21"}},"metadata":{}},{"name":"stdout","text":"Preprocessing validation dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/857 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f2d627bc6134c518337cfc492f140ca"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ================================\n# 5. DATA COLLATOR FOR NOISE-ROBUST TRAINING\n# ================================\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    \n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Handle input features\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n        \n        # Handle labels\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n        \n        # Replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        \n        # Remove BOS token if present\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n        \n        batch[\"labels\"] = labels\n        return batch\n\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:26:46.095629Z","iopub.execute_input":"2025-07-12T04:26:46.095929Z","iopub.status.idle":"2025-07-12T04:26:46.102769Z","shell.execute_reply.started":"2025-07-12T04:26:46.095901Z","shell.execute_reply":"2025-07-12T04:26:46.102165Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ================================\n# 6. METRICS FOR NOISE-ROBUST EVALUATION\n# ================================\n\ndef compute_metrics(pred):\n    \"\"\"Compute comprehensive metrics for noisy ASR evaluation\"\"\"\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    # Replace -100 with pad_token_id\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n    # Decode token IDs to strings\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    # Calculate multiple metrics\n    wer_score = wer(label_str, pred_str) * 100\n    cer_score = cer(label_str, pred_str) * 100\n    mer_score = mer(label_str, pred_str) * 100\n\n    # Sentence Error Rate\n    ser_score = (\n        sum(ref.strip() != pred.strip() for ref, pred in zip(label_str, pred_str))\n        / len(label_str)\n    ) * 100\n\n    return {\n        \"wer\": wer_score,\n        \"cer\": cer_score,\n        \"ter\": mer_score,\n        \"ser\": ser_score,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:26:46.103490Z","iopub.execute_input":"2025-07-12T04:26:46.103656Z","iopub.status.idle":"2025-07-12T04:26:46.130027Z","shell.execute_reply.started":"2025-07-12T04:26:46.103641Z","shell.execute_reply":"2025-07-12T04:26:46.129432Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ================================\n# 7. MODEL SETUP FOR NOISE-ROBUST TRAINING\n# ================================\nprint(\"Loading pre-trained Whisper model...\")\ntry:\n    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n    print(\"Model loaded successfully!\")\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n    raise\n\n# Freeze encoder for noise robustness (optional - uncomment if needed)\n# for param in model.model.encoder.parameters():\n#     param.requires_grad = False\n\n# Verify model is loaded\nprint(f\"Model type: {type(model)}\")\nprint(f\"Model config: {model.config}\")\nprint(f\"Model device: {next(model.parameters()).device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:26:46.130745Z","iopub.execute_input":"2025-07-12T04:26:46.130966Z","iopub.status.idle":"2025-07-12T04:26:48.469236Z","shell.execute_reply.started":"2025-07-12T04:26:46.130949Z","shell.execute_reply":"2025-07-12T04:26:48.468381Z"}},"outputs":[{"name":"stdout","text":"Loading pre-trained Whisper model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ad58a933ef480990f334e114720991"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9303075f799b4b0c825be113ce65eb7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f70e12aa615b4fbebc88ed1468e3615b"}},"metadata":{}},{"name":"stdout","text":"Model loaded successfully!\nModel type: <class 'transformers.models.whisper.modeling_whisper.WhisperForConditionalGeneration'>\nModel config: WhisperConfig {\n  \"_attn_implementation_autoset\": true,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"apply_spec_augment\": false,\n  \"architectures\": [\n    \"WhisperForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"begin_suppress_tokens\": [\n    220,\n    50257\n  ],\n  \"bos_token_id\": 50257,\n  \"classifier_proj_size\": 256,\n  \"d_model\": 512,\n  \"decoder_attention_heads\": 8,\n  \"decoder_ffn_dim\": 2048,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 50258,\n  \"dropout\": 0.0,\n  \"encoder_attention_heads\": 8,\n  \"encoder_ffn_dim\": 2048,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 50257,\n  \"forced_decoder_ids\": [\n    [\n      1,\n      50259\n    ],\n    [\n      2,\n      50359\n    ],\n    [\n      3,\n      50363\n    ]\n  ],\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"mask_feature_length\": 10,\n  \"mask_feature_min_masks\": 0,\n  \"mask_feature_prob\": 0.0,\n  \"mask_time_length\": 10,\n  \"mask_time_min_masks\": 2,\n  \"mask_time_prob\": 0.05,\n  \"max_length\": 448,\n  \"max_source_positions\": 1500,\n  \"max_target_positions\": 448,\n  \"median_filter_width\": 7,\n  \"model_type\": \"whisper\",\n  \"num_hidden_layers\": 6,\n  \"num_mel_bins\": 80,\n  \"pad_token_id\": 50257,\n  \"scale_embedding\": false,\n  \"suppress_tokens\": [\n    1,\n    2,\n    7,\n    8,\n    9,\n    10,\n    14,\n    25,\n    26,\n    27,\n    28,\n    29,\n    31,\n    58,\n    59,\n    60,\n    61,\n    62,\n    63,\n    90,\n    91,\n    92,\n    93,\n    359,\n    503,\n    522,\n    542,\n    873,\n    893,\n    902,\n    918,\n    922,\n    931,\n    1350,\n    1853,\n    1982,\n    2460,\n    2627,\n    3246,\n    3253,\n    3268,\n    3536,\n    3846,\n    3961,\n    4183,\n    4667,\n    6585,\n    6647,\n    7273,\n    9061,\n    9383,\n    10428,\n    10929,\n    11938,\n    12033,\n    12331,\n    12562,\n    13793,\n    14157,\n    14635,\n    15265,\n    15618,\n    16553,\n    16604,\n    18362,\n    18956,\n    20075,\n    21675,\n    22520,\n    26130,\n    26161,\n    26435,\n    28279,\n    29464,\n    31650,\n    32302,\n    32470,\n    36865,\n    42863,\n    47425,\n    49870,\n    50254,\n    50258,\n    50358,\n    50359,\n    50360,\n    50361,\n    50362\n  ],\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"use_weighted_layer_sum\": false,\n  \"vocab_size\": 51865\n}\n\nModel device: cpu\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ================================\n# 8. TRAINING ARGUMENTS FOR NOISY DATA\n# ================================\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-base-ta-noisy-robust\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    \n    # Evaluation and saving\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    \n    # Batch sizes - adjusted for noisy data\n    per_device_train_batch_size=8,  # Reduced for stability\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,   # Compensate for baseer batch size\n    \n    # Learning rate - slightly lower for noisy data\n    learning_rate=3e-5,\n    warmup_steps=500,\n    lr_scheduler_type=\"linear\",\n    \n    # Memory optimization\n    gradient_checkpointing=True,\n    fp16=True,\n    dataloader_pin_memory=False,\n    \n    # Training duration\n    num_train_epochs=10,  # Reduced epochs for noisy data\n    \n    # Generation settings\n    predict_with_generate=True,\n    generation_max_length=225,\n    \n    # Logging\n    logging_steps=50,\n    report_to=[\"tensorboard\"],\n    \n    # Additional stability settings\n    max_grad_norm=1.0,\n    weight_decay=0.01,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:32:25.517979Z","iopub.execute_input":"2025-07-12T04:32:25.518311Z","iopub.status.idle":"2025-07-12T04:32:25.553798Z","shell.execute_reply.started":"2025-07-12T04:32:25.518290Z","shell.execute_reply":"2025-07-12T04:32:25.553177Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# # ================================\n# # 9. TRAINER SETUP\n# # ================================\n\n# from transformers import WhisperForConditionalGeneration, WhisperConfig, EarlyStoppingCallback\n\n# # Step 1: Load config and increase dropout\n# config = WhisperConfig.from_pretrained(\"openai/whisper-base\")\n# config.dropout = 0.2\n\n# # Step 2: Load model with new config\n# model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\", config=config)\n\n# # Step 3: Add EarlyStopping (optional but recommended)\n# callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n\n# # Step 4: Use the model and training_args in your trainer\n# trainer = Seq2SeqTrainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,\n#     tokenizer=processor.feature_extractor,\n#     compute_metrics=compute_metrics,\n#     callbacks=callbacks\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:32:26.906416Z","iopub.execute_input":"2025-07-12T04:32:26.907062Z","iopub.status.idle":"2025-07-12T04:32:26.910631Z","shell.execute_reply.started":"2025-07-12T04:32:26.907037Z","shell.execute_reply":"2025-07-12T04:32:26.910019Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# ================================\n# 9. TRAINER SETUP\n# ================================\n# Step 1: Load config and increase dropout\nconfig = WhisperConfig.from_pretrained(\"openai/whisper-base\")\nconfig.dropout = 0.2\n\n# Step 2: Load model with new config\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\", config=config)\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:32:28.412522Z","iopub.execute_input":"2025-07-12T04:32:28.413244Z","iopub.status.idle":"2025-07-12T04:32:28.848083Z","shell.execute_reply.started":"2025-07-12T04:32:28.413207Z","shell.execute_reply":"2025-07-12T04:32:28.847155Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/2475419867.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# # ================================\n# # 10. TRAINING EXECUTION\n# # ================================\n\n# print(\"Starting noise-robust ASR training...\")\n# print(f\"Training on {len(train_dataset)} noisy samples\")\n# print(f\"Validating on {len(val_dataset)} noisy samples\")\n\n# # Train the model\n# trainer.train(resume_from_checkpoint=\"/kaggle/input/last-checkpoint-001\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:32:33.321663Z","iopub.execute_input":"2025-07-12T04:32:33.321975Z","iopub.status.idle":"2025-07-12T04:32:33.325530Z","shell.execute_reply.started":"2025-07-12T04:32:33.321953Z","shell.execute_reply":"2025-07-12T04:32:33.324772Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# ================================\n# 10. TRAINING EXECUTION\n# ================================\n\nprint(\"Starting noise-robust ASR training...\")\nprint(f\"Training on {len(train_dataset)} noisy samples\")\nprint(f\"Validating on {len(val_dataset)} noisy samples\")\n\n# Train the model\ntrainer.train()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:32:37.318297Z","iopub.execute_input":"2025-07-12T04:32:37.318575Z","iopub.status.idle":"2025-07-12T07:44:48.093149Z","shell.execute_reply.started":"2025-07-12T04:32:37.318554Z","shell.execute_reply":"2025-07-12T07:44:48.092364Z"}},"outputs":[{"name":"stdout","text":"Starting noise-robust ASR training...\nTraining on 3427 noisy samples\nValidating on 857 noisy samples\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='972' max='1070' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 972/1070 3:11:57 < 19:23, 0.08 it/s, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Wer</th>\n      <th>Cer</th>\n      <th>Ter</th>\n      <th>Ser</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.586600</td>\n      <td>1.886346</td>\n      <td>123.027211</td>\n      <td>317.691837</td>\n      <td>97.231183</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.913500</td>\n      <td>2.571924</td>\n      <td>567.278912</td>\n      <td>385.255528</td>\n      <td>99.922114</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.606300</td>\n      <td>2.480611</td>\n      <td>137.959184</td>\n      <td>392.714788</td>\n      <td>99.987674</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.438800</td>\n      <td>2.357719</td>\n      <td>103.826531</td>\n      <td>77.974141</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.299400</td>\n      <td>2.184037</td>\n      <td>101.020408</td>\n      <td>79.144671</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.211700</td>\n      <td>2.122048</td>\n      <td>100.323129</td>\n      <td>79.114069</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.128600</td>\n      <td>2.060203</td>\n      <td>103.435374</td>\n      <td>87.407237</td>\n      <td>99.950698</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.087100</td>\n      <td>2.029236</td>\n      <td>125.357143</td>\n      <td>102.507459</td>\n      <td>99.959317</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.073800</td>\n      <td>2.018514</td>\n      <td>126.547619</td>\n      <td>108.909035</td>\n      <td>99.973129</td>\n      <td>100.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=972, training_loss=2.918557575210132, metrics={'train_runtime': 11530.1485, 'train_samples_per_second': 2.972, 'train_steps_per_second': 0.093, 'total_flos': 2.00047686156288e+18, 'train_loss': 2.918557575210132, 'epoch': 9.0})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Save the final model\ntrainer.save_model(\"./whisper-base-ta-noisy-robust-final\")\nprocessor.save_pretrained(\"./whisper-base-ta-noisy-robust-final\")\n\nprint(\"Training completed!\")\nprint(\"Model saved to ./whisper-base-ta-noisy-robust-final\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# 12. CREATE DOWNLOADABLE ARCHIVES FOR KAGGLE\n# ================================\n\nimport os\nimport shutil\nimport zipfile\nfrom IPython.display import FileLink\n\ndef create_downloadable_archive(source_dir, archive_name):\n    \"\"\"Create a downloadable zip archive\"\"\"\n    if os.path.exists(source_dir):\n        # Create zip file\n        shutil.make_archive(archive_name, 'zip', source_dir)\n        zip_path = f\"{archive_name}.zip\"\n        \n        if os.path.exists(zip_path):\n            file_size = os.path.getsize(zip_path) / (1024 * 1024)  # Size in MB\n            print(f\"✅ Created {zip_path} ({file_size:.2f} MB)\")\n            return zip_path\n        else:\n            print(f\"❌ Failed to create {zip_path}\")\n            return None\n    else:\n        print(f\"❌ Source directory {source_dir} does not exist\")\n        return None\n\n# Create downloadable archives\nprint(\"\\n🔄 Creating downloadable model archives...\")\n\n# 1. Final trained model\nfinal_model_zip = create_downloadable_archive(\n    \"./whisper-base-ta-noisy-robust-final\", \n    \"whisper-base-ta-noisy-robust-final\"\n)\n\n# 2. Last checkpoint from training\ncheckpoint_dir = \"./whisper-base-ta-noisy-robust\"\nif os.path.exists(checkpoint_dir):\n    # Find the last checkpoint\n    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n    if checkpoints:\n        # Sort by checkpoint number\n        checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n        last_checkpoint = checkpoints[-1]\n        last_checkpoint_path = os.path.join(checkpoint_dir, last_checkpoint)\n        \n        print(f\"📂 Found last checkpoint: {last_checkpoint}\")\n        \n        # Create archive for last checkpoint\n        last_checkpoint_zip = create_downloadable_archive(\n            last_checkpoint_path, \n            f\"whisper-base-ta-noisy-robust-{last_checkpoint}\"\n        )\n    else:\n        print(\"❌ No checkpoints found in training directory\")\n        last_checkpoint_zip = None\nelse:\n    print(\"❌ Training directory does not exist\")\n    last_checkpoint_zip = None\n\n# 3. Training logs and metrics\nif os.path.exists(\"./whisper-base-ta-noisy-robust/runs\"):\n    tensorboard_logs_zip = create_downloadable_archive(\n        \"./whisper-base-ta-noisy-robust/runs\", \n        \"whisper-base-ta-noisy-robust-tensorboard-logs\"\n    )\nelse:\n    tensorboard_logs_zip = None\n\n# Create a comprehensive package with all files\nprint(\"\\n📦 Creating comprehensive model package...\")\ncomprehensive_package = \"whisper-base-ta-noisy-robust-complete\"\nos.makedirs(comprehensive_package, exist_ok=True)\n\n# Copy final model\nif os.path.exists(\"./whisper-base-ta-noisy-robust-final\"):\n    shutil.copytree(\"./whisper-base-ta-noisy-robust-final\", \n                    f\"{comprehensive_package}/final_model\", \n                    dirs_exist_ok=True)\n\n# Copy last checkpoint\nif os.path.exists(last_checkpoint_path):\n    shutil.copytree(last_checkpoint_path, \n                    f\"{comprehensive_package}/last_checkpoint\", \n                    dirs_exist_ok=True)\n\n# Copy training data splits\nif os.path.exists(\"train.csv\"):\n    shutil.copy(\"train.csv\", f\"{comprehensive_package}/train.csv\")\nif os.path.exists(\"val.csv\"):\n    shutil.copy(\"val.csv\", f\"{comprehensive_package}/val.csv\")\n\n# Create README file\nreadme_content = f\"\"\"# Whisper base Tamil Noisy-Robust ASR Model\n\n## Training Information\n- base Model: openai/whisper-base\n- Language: Tamil (ta)\n- Task: Transcription\n- Dataset: Noisy audio data for ASR\n- Training Strategy: Noise-robust fine-tuning\n\n## Model Files\n- `final_model/`: Complete trained model ready for inference\n- `last_checkpoint/`: Last training checkpoint\n- `train.csv`: Training data split\n- `val.csv`: Validation data split\n\n## Usage\n```python\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor\n\n# Load the model\nmodel = WhisperForConditionalGeneration.from_pretrained(\"./final_model\")\nprocessor = WhisperProcessor.from_pretrained(\"./final_model\")\n\n# Use for inference\n# (Add your inference code here)\n```\n\n## Training Configuration\n- Batch Size: 16 (per device)\n- Learning Rate: 1e-5\n- Epochs: 15\n- Early Stopping: 3 epochs patience\n- Optimizer: AdamW with weight decay\n\n## Evaluation Metrics\n- WER (Word Error Rate)\n- CER (Character Error Rate)\n- SER (Sentence Error Rate)\n- TER (Token Error Rate)\n\nTraining completed successfully!\n\"\"\"\n\nwith open(f\"{comprehensive_package}/README.md\", \"w\", encoding=\"utf-8\") as f:\n    f.write(readme_content)\n\n# Create comprehensive zip\ncomprehensive_zip = create_downloadable_archive(\n    comprehensive_package, \n    \"whisper-base-ta-noisy-robust-complete\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:26:52.016597Z","iopub.status.idle":"2025-07-12T04:26:52.017167Z","shell.execute_reply.started":"2025-07-12T04:26:52.017014Z","shell.execute_reply":"2025-07-12T04:26:52.017037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# 13. DISPLAY DOWNLOAD LINKS\n# ================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"📥 DOWNLOAD LINKS FOR KAGGLE\")\nprint(\"=\"*60)\n\ndownload_links = []\n\nif final_model_zip:\n    download_links.append((\"Final Trained Model\", final_model_zip))\n    print(f\"🎯 Final Model: {final_model_zip}\")\n\nif last_checkpoint_zip:\n    download_links.append((\"Last Checkpoint\", last_checkpoint_zip))\n    print(f\"📍 Last Checkpoint: {last_checkpoint_zip}\")\n\nif tensorboard_logs_zip:\n    download_links.append((\"TensorBoard Logs\", tensorboard_logs_zip))\n    print(f\"📊 TensorBoard Logs: {tensorboard_logs_zip}\")\n\nif comprehensive_zip:\n    download_links.append((\"Complete Package\", comprehensive_zip))\n    print(f\"📦 Complete Package: {comprehensive_zip}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"🔗 DIRECT DOWNLOAD COMMANDS\")\nprint(\"=\"*60)\n\n# Create download commands for each file\nfor name, file_path in download_links:\n    if file_path and os.path.exists(file_path):\n        print(f\"\\n# Download {name}\")\n        print(f\"# File: {file_path}\")\n        print(f\"# Size: {os.path.getsize(file_path) / (1024*1024):.2f} MB\")\n        \n        # For Kaggle, files in the working directory are automatically available for download\n        # Just need to display the file link\n        display(FileLink(file_path))\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"✅ All model files are ready for download!\")\nprint(\"📝 Click the links above to download the files\")\nprint(\"💡 The 'Complete Package' contains everything you need\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T04:26:52.018195Z","iopub.status.idle":"2025-07-12T04:26:52.018529Z","shell.execute_reply.started":"2025-07-12T04:26:52.018360Z","shell.execute_reply":"2025-07-12T04:26:52.018380Z"}},"outputs":[],"execution_count":null}]}