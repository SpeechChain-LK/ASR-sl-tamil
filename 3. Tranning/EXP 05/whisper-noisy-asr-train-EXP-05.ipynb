{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-13T08:52:29.649618Z",
     "iopub.status.busy": "2025-07-13T08:52:29.648979Z",
     "iopub.status.idle": "2025-07-13T08:54:10.273706Z",
     "shell.execute_reply": "2025-07-13T08:54:10.272783Z",
     "shell.execute_reply.started": "2025-07-13T08:52:29.649593Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, fsspec, jiwer, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.5 fsspec-2025.3.0 jiwer-4.0.0 rapidfuzz-3.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2.4.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.3->librosa) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.3->librosa) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.3->librosa) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.3->librosa) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.3->librosa) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.26.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->soundfile) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->soundfile) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->soundfile) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->soundfile) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->soundfile) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->soundfile) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.72.0rc1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.12.0->tensorboard) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.12.0->tensorboard) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.12.0->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.12.0->tensorboard) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.12.0->tensorboard) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install transformers datasets evaluate jiwer\n",
    "%pip install librosa scikit-learn pandas\n",
    "%pip install soundfile\n",
    "%pip install tensorboard\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:54:10.275673Z",
     "iopub.status.busy": "2025-07-13T08:54:10.275440Z",
     "iopub.status.idle": "2025-07-13T08:54:48.825819Z",
     "shell.execute_reply": "2025-07-13T08:54:48.825241Z",
     "shell.execute_reply.started": "2025-07-13T08:54:10.275650Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 08:54:31.003771: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752396871.345085      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752396871.445415      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor, \n",
    "    WhisperTokenizer, \n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "from jiwer import wer, cer, mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:54:48.827131Z",
     "iopub.status.busy": "2025-07-13T08:54:48.826515Z",
     "iopub.status.idle": "2025-07-13T08:54:48.957595Z",
     "shell.execute_reply": "2025-07-13T08:54:48.957002Z",
     "shell.execute_reply.started": "2025-07-13T08:54:48.827104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4284 samples\n",
      "Sample data:\n",
      "                                ID  \\\n",
      "0  taf_02345_00348037167_noisy.wav   \n",
      "1  taf_07049_00155837462_noisy.wav   \n",
      "2  taf_09705_01218130267_noisy.wav   \n",
      "3  taf_03219_00712757493_noisy.wav   \n",
      "4  taf_00008_01305524612_noisy.wav   \n",
      "\n",
      "                                            sentence  \\\n",
      "0  ஆஸ்த்ரேலியப் பெண்ணுக்கு முப்பத்தி மூன்று ஆண்டு...   \n",
      "1            ஸ்ரீரங்கம் கோவிலில் வெடிகுண்டு மிரட்டல்   \n",
      "2  உங்களுடைய உணவுக் கட்டுப்பாட்டைச் சொன்னால் மற்ற...   \n",
      "3  ரஹானே மட்டும் ஆறுதலாக முப்பத்தி ஆறு ரன்கள் குவ...   \n",
      "4               மனோரமாவிற்கு பூபதி என்ற மகன் உள்ளார்   \n",
      "\n",
      "                                               audio  \n",
      "0  /kaggle/input/noisy-audio-data-for-asr/noisy-a...  \n",
      "1  /kaggle/input/noisy-audio-data-for-asr/noisy-a...  \n",
      "2  /kaggle/input/noisy-audio-data-for-asr/noisy-a...  \n",
      "3  /kaggle/input/noisy-audio-data-for-asr/noisy-a...  \n",
      "4  /kaggle/input/noisy-audio-data-for-asr/noisy-a...  \n",
      "Training samples: 3427\n",
      "Validation samples: 857\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 1. DATA PREPARATION\n",
    "# ================================\n",
    "\n",
    "# Load the dataset from the TSV file\n",
    "file_path = \"/kaggle/input/noisy-data/noisy_data.tsv\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"ID\", \"Text\"])\n",
    "\n",
    "# Add full path to audio files\n",
    "df[\"audio\"] = df[\"ID\"].apply(lambda x: f\"/kaggle/input/noisy-audio-data-for-asr/noisy-audio-data-for-ASR/{x}\")\n",
    "df = df.rename(columns={\"Text\": \"sentence\"})\n",
    "\n",
    "# Remove any rows with missing audio files or text\n",
    "df = df.dropna()\n",
    "df = df[df[\"sentence\"].str.strip() != \"\"]\n",
    "\n",
    "print(f\"Dataset size: {len(df)} samples\")\n",
    "print(f\"Sample data:\\n{df.head()}\")\n",
    "\n",
    "# Train-validation split with stratification if needed\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Save splits for reference\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:54:48.959249Z",
     "iopub.status.busy": "2025-07-13T08:54:48.959013Z",
     "iopub.status.idle": "2025-07-13T08:56:08.014361Z",
     "shell.execute_reply": "2025-07-13T08:56:08.013686Z",
     "shell.execute_reply.started": "2025-07-13T08:54:48.959231Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating audio files...\n",
      "Valid training samples: 3427\n",
      "Valid validation samples: 857\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 2. AUDIO PREPROCESSING & AUGMENTATION\n",
    "# ================================\n",
    "\n",
    "def verify_audio_file(audio_path):\n",
    "    \"\"\"Verify if audio file exists and is readable\"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=16000)\n",
    "        return len(audio) > 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def add_noise_augmentation(audio_array, noise_factor=0.005):\n",
    "    \"\"\"Add Gaussian noise for additional robustness\"\"\"\n",
    "    noise = np.random.normal(0, noise_factor, audio_array.shape)\n",
    "    return audio_array + noise\n",
    "\n",
    "def normalize_audio(audio_array):\n",
    "    \"\"\"Normalize audio to prevent clipping\"\"\"\n",
    "    max_val = np.max(np.abs(audio_array))\n",
    "    if max_val > 0:\n",
    "        return audio_array / max_val\n",
    "    return audio_array\n",
    "\n",
    "# Filter out invalid audio files\n",
    "valid_train = []\n",
    "valid_val = []\n",
    "\n",
    "print(\"Validating audio files...\")\n",
    "for idx, row in train_df.iterrows():\n",
    "    if verify_audio_file(row['audio']):\n",
    "        valid_train.append(row)\n",
    "    else:\n",
    "        print(f\"Invalid audio file: {row['audio']}\")\n",
    "\n",
    "for idx, row in val_df.iterrows():\n",
    "    if verify_audio_file(row['audio']):\n",
    "        valid_val.append(row)\n",
    "    else:\n",
    "        print(f\"Invalid audio file: {row['audio']}\")\n",
    "\n",
    "train_df = pd.DataFrame(valid_train)\n",
    "val_df = pd.DataFrame(valid_val)\n",
    "\n",
    "print(f\"Valid training samples: {len(train_df)}\")\n",
    "print(f\"Valid validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:56:08.015670Z",
     "iopub.status.busy": "2025-07-13T08:56:08.015076Z",
     "iopub.status.idle": "2025-07-13T08:56:10.131754Z",
     "shell.execute_reply": "2025-07-13T08:56:10.131158Z",
     "shell.execute_reply.started": "2025-07-13T08:56:08.015647Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbaddf99d6a4962a7cefe19a9bca954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b648ad81e6dd48fb931be68235901c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0143a492b1407e96b8181e36cfaf86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af36cd8f4b4941689dd0d395db3fa6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d579f61b84c460aa72fb4bb10243efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dfbe84c2a04e0c998cad2ef3271b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdf2e7bb72643f696888ade99058e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce897f35421422c80376c230a7ccf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================\n",
    "# 3. WHISPER PROCESSOR SETUP\n",
    "# ================================\n",
    "\n",
    "# Initialize Whisper components for Tamil\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"ta\", task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"ta\", task=\"transcribe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:56:10.132649Z",
     "iopub.status.busy": "2025-07-13T08:56:10.132464Z",
     "iopub.status.idle": "2025-07-13T08:57:19.131393Z",
     "shell.execute_reply": "2025-07-13T08:57:19.130638Z",
     "shell.execute_reply.started": "2025-07-13T08:56:10.132635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1bd0a618a8479092bd4851563bac24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff505998d8ac4e6d9b9634431734969b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================\n",
    "# 4. DATASET CREATION WITH NOISE HANDLING\n",
    "# ================================\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Cast audio with target sampling rate\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "def prepare_dataset(examples):\n",
    "    \"\"\"Prepare dataset with noise-robust preprocessing\"\"\"\n",
    "    # Load and process audio\n",
    "    audio = examples[\"audio\"]\n",
    "    audio_array = audio[\"array\"]\n",
    "    \n",
    "    # Apply noise-robust preprocessing\n",
    "    audio_array = normalize_audio(audio_array)\n",
    "    \n",
    "    # Optional: Add slight additional noise for robustness (uncomment if needed)\n",
    "    # audio_array = add_noise_augmentation(audio_array, noise_factor=0.001)\n",
    "    \n",
    "    # Ensure audio is not too short or too long\n",
    "    min_length = 1000  # ~0.06 seconds at 16kHz\n",
    "    max_length = 480000  # ~30 seconds at 16kHz\n",
    "    \n",
    "    if len(audio_array) < min_length:\n",
    "        # Pad short audio\n",
    "        audio_array = np.pad(audio_array, (0, min_length - len(audio_array)), 'constant')\n",
    "    elif len(audio_array) > max_length:\n",
    "        # Truncate long audio\n",
    "        audio_array = audio_array[:max_length]\n",
    "    \n",
    "    # Compute log-Mel input features\n",
    "    examples[\"input_features\"] = feature_extractor(\n",
    "        audio_array, sampling_rate=16000\n",
    "    ).input_features[0]\n",
    "    \n",
    "    # Clean up\n",
    "    del examples[\"audio\"]\n",
    "    \n",
    "    # Process text\n",
    "    sentences = examples[\"sentence\"]\n",
    "    \n",
    "    # Clean and normalize text\n",
    "    if isinstance(sentences, str):\n",
    "        sentences = sentences.strip()\n",
    "    \n",
    "    # Encode target text to label ids\n",
    "    examples[\"labels\"] = tokenizer(sentences).input_ids\n",
    "    del examples[\"sentence\"]\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing training dataset...\")\n",
    "train_dataset = train_dataset.map(prepare_dataset, num_proc=1)\n",
    "\n",
    "print(\"Preprocessing validation dataset...\")\n",
    "val_dataset = val_dataset.map(prepare_dataset, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:57:19.132659Z",
     "iopub.status.busy": "2025-07-13T08:57:19.132342Z",
     "iopub.status.idle": "2025-07-13T08:57:19.139336Z",
     "shell.execute_reply": "2025-07-13T08:57:19.138682Z",
     "shell.execute_reply.started": "2025-07-13T08:57:19.132631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 5. DATA COLLATOR FOR NOISE-ROBUST TRAINING\n",
    "# ================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Handle input features\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        \n",
    "        # Handle labels\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        \n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        # Remove BOS token if present\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:57:19.140265Z",
     "iopub.status.busy": "2025-07-13T08:57:19.140012Z",
     "iopub.status.idle": "2025-07-13T08:57:19.166184Z",
     "shell.execute_reply": "2025-07-13T08:57:19.165710Z",
     "shell.execute_reply.started": "2025-07-13T08:57:19.140248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 6. METRICS FOR NOISE-ROBUST EVALUATION\n",
    "# ================================\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute comprehensive metrics for noisy ASR evaluation\"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decode token IDs to strings\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Calculate multiple metrics\n",
    "    wer_score = wer(label_str, pred_str) * 100\n",
    "    cer_score = cer(label_str, pred_str) * 100\n",
    "    mer_score = mer(label_str, pred_str) * 100\n",
    "\n",
    "    # Sentence Error Rate\n",
    "    ser_score = (\n",
    "        sum(ref.strip() != pred.strip() for ref, pred in zip(label_str, pred_str))\n",
    "        / len(label_str)\n",
    "    ) * 100\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer_score,\n",
    "        \"cer\": cer_score,\n",
    "        \"ter\": mer_score,\n",
    "        \"ser\": ser_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:57:19.167188Z",
     "iopub.status.busy": "2025-07-13T08:57:19.166950Z",
     "iopub.status.idle": "2025-07-13T08:57:26.691646Z",
     "shell.execute_reply": "2025-07-13T08:57:26.690886Z",
     "shell.execute_reply.started": "2025-07-13T08:57:19.167163Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained Whisper model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1c42dd161a4efeb85b44c9634ec095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508e46879f5a40ca94fbebd4309c4260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8356d4511e4123b79c74d86e187e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model type: <class 'transformers.models.whisper.modeling_whisper.WhisperForConditionalGeneration'>\n",
      "Model config: WhisperConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"apply_spec_augment\": false,\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"median_filter_width\": 7,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 7. MODEL SETUP FOR NOISE-ROBUST TRAINING\n",
    "# ================================\n",
    "print(\"Loading pre-trained Whisper model...\")\n",
    "try:\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Freeze encoder for noise robustness (optional - uncomment if needed)\n",
    "# for param in model.model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Verify model is loaded\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Model config: {model.config}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:57:26.694005Z",
     "iopub.status.busy": "2025-07-13T08:57:26.693804Z",
     "iopub.status.idle": "2025-07-13T08:57:26.738350Z",
     "shell.execute_reply": "2025-07-13T08:57:26.737423Z",
     "shell.execute_reply.started": "2025-07-13T08:57:26.693990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 8. TRAINING ARGUMENTS FOR NOISY DATA\n",
    "# ================================\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-ta-noisy-robust\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Batch sizes - adjusted for noisy data\n",
    "    per_device_train_batch_size=16,  # Reduced for stability\n",
    "    per_device_eval_batch_size=16,\n",
    "    # gradient_accumulation_steps=1,   # Compensate for smaller batch size\n",
    "    \n",
    "    # Learning rate - slightly lower for noisy data\n",
    "    learning_rate=1.7e-05,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=10,  # Reduced epochs for noisy data\n",
    "    \n",
    "    # Generation settings\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    \n",
    "    # Additional stability settings\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:57:26.739850Z",
     "iopub.status.busy": "2025-07-13T08:57:26.739534Z",
     "iopub.status.idle": "2025-07-13T08:57:26.945828Z",
     "shell.execute_reply": "2025-07-13T08:57:26.945309Z",
     "shell.execute_reply.started": "2025-07-13T08:57:26.739825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ================================\n",
    "# # 9. TRAINER SETUP\n",
    "# # ================================\n",
    "\n",
    "# from transformers import WhisperForConditionalGeneration, WhisperConfig, EarlyStoppingCallback\n",
    "\n",
    "# # Step 1: Load config and increase dropout\n",
    "# config = WhisperConfig.from_pretrained(\"openai/whisper-small\")\n",
    "# config.dropout = 0.2\n",
    "\n",
    "# # Step 2: Load model with new config\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", config=config)\n",
    "\n",
    "# # Step 3: Add EarlyStopping (optional but recommended)\n",
    "# callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "\n",
    "# # Step 4: Use the model and training_args in your trainer\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     tokenizer=processor.feature_extractor,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=callbacks\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:57:26.946869Z",
     "iopub.status.busy": "2025-07-13T08:57:26.946521Z",
     "iopub.status.idle": "2025-07-13T08:57:27.916987Z",
     "shell.execute_reply": "2025-07-13T08:57:27.916434Z",
     "shell.execute_reply.started": "2025-07-13T08:57:26.946846Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/347744420.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 9. TRAINER SETUP\n",
    "# ================================\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:57:27.917837Z",
     "iopub.status.busy": "2025-07-13T08:57:27.917638Z",
     "iopub.status.idle": "2025-07-13T08:57:27.920998Z",
     "shell.execute_reply": "2025-07-13T08:57:27.920470Z",
     "shell.execute_reply.started": "2025-07-13T08:57:27.917823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ================================\n",
    "# # 10. TRAINING EXECUTION\n",
    "# # ================================\n",
    "\n",
    "# print(\"Starting noise-robust ASR training...\")\n",
    "# print(f\"Training on {len(train_dataset)} noisy samples\")\n",
    "# print(f\"Validating on {len(val_dataset)} noisy samples\")\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train(resume_from_checkpoint=\"/kaggle/input/last-checkpoint-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:57:27.921939Z",
     "iopub.status.busy": "2025-07-13T08:57:27.921743Z",
     "iopub.status.idle": "2025-07-13T12:27:32.544183Z",
     "shell.execute_reply": "2025-07-13T12:27:32.543592Z",
     "shell.execute_reply.started": "2025-07-13T08:57:27.921925Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting noise-robust ASR training...\n",
      "Training on 3427 noisy samples\n",
      "Validating on 857 noisy samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 432/1080 3:29:36 < 5:15:52, 0.03 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "      <th>Ter</th>\n",
       "      <th>Ser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.443500</td>\n",
       "      <td>0.265852</td>\n",
       "      <td>48.180272</td>\n",
       "      <td>11.252008</td>\n",
       "      <td>46.564760</td>\n",
       "      <td>91.948658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.152600</td>\n",
       "      <td>0.180565</td>\n",
       "      <td>78.010204</td>\n",
       "      <td>168.435850</td>\n",
       "      <td>71.017185</td>\n",
       "      <td>91.831972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.167386</td>\n",
       "      <td>62.704082</td>\n",
       "      <td>143.280927</td>\n",
       "      <td>60.323953</td>\n",
       "      <td>86.697783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.172607</td>\n",
       "      <td>83.945578</td>\n",
       "      <td>220.845765</td>\n",
       "      <td>81.170860</td>\n",
       "      <td>92.065344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=432, training_loss=0.2276672733326753, metrics={'train_runtime': 12604.0555, 'train_samples_per_second': 2.719, 'train_steps_per_second': 0.086, 'total_flos': 3.95592866758656e+18, 'train_loss': 0.2276672733326753, 'epoch': 4.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================\n",
    "# 10. TRAINING EXECUTION\n",
    "# ================================\n",
    "\n",
    "print(\"Starting noise-robust ASR training...\")\n",
    "print(f\"Training on {len(train_dataset)} noisy samples\")\n",
    "print(f\"Validating on {len(val_dataset)} noisy samples\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T12:27:32.545191Z",
     "iopub.status.busy": "2025-07-13T12:27:32.544931Z",
     "iopub.status.idle": "2025-07-13T12:27:34.396226Z",
     "shell.execute_reply": "2025-07-13T12:27:34.395522Z",
     "shell.execute_reply.started": "2025-07-13T12:27:32.545172Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Model saved to ./whisper-small-ta-noisy-robust-final\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "trainer.save_model(\"./whisper-small-ta-noisy-robust-final\")\n",
    "processor.save_pretrained(\"./whisper-small-ta-noisy-robust-final\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(\"Model saved to ./whisper-small-ta-noisy-robust-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T12:27:34.397540Z",
     "iopub.status.busy": "2025-07-13T12:27:34.397269Z",
     "iopub.status.idle": "2025-07-13T12:34:17.000897Z",
     "shell.execute_reply": "2025-07-13T12:34:17.000260Z",
     "shell.execute_reply.started": "2025-07-13T12:27:34.397522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Creating downloadable model archives...\n",
      "✅ Created whisper-small-ta-noisy-robust-final.zip (853.71 MB)\n",
      "📂 Found last checkpoint: checkpoint-432\n",
      "✅ Created whisper-small-ta-noisy-robust-checkpoint-432.zip (2552.72 MB)\n",
      "✅ Created whisper-small-ta-noisy-robust-tensorboard-logs.zip (0.00 MB)\n",
      "\n",
      "📦 Creating comprehensive model package...\n",
      "✅ Created whisper-small-ta-noisy-robust-complete.zip (3406.64 MB)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 12. CREATE DOWNLOADABLE ARCHIVES FOR KAGGLE\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from IPython.display import FileLink\n",
    "\n",
    "def create_downloadable_archive(source_dir, archive_name):\n",
    "    \"\"\"Create a downloadable zip archive\"\"\"\n",
    "    if os.path.exists(source_dir):\n",
    "        # Create zip file\n",
    "        shutil.make_archive(archive_name, 'zip', source_dir)\n",
    "        zip_path = f\"{archive_name}.zip\"\n",
    "        \n",
    "        if os.path.exists(zip_path):\n",
    "            file_size = os.path.getsize(zip_path) / (1024 * 1024)  # Size in MB\n",
    "            print(f\"✅ Created {zip_path} ({file_size:.2f} MB)\")\n",
    "            return zip_path\n",
    "        else:\n",
    "            print(f\"❌ Failed to create {zip_path}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"❌ Source directory {source_dir} does not exist\")\n",
    "        return None\n",
    "\n",
    "# Create downloadable archives\n",
    "print(\"\\n🔄 Creating downloadable model archives...\")\n",
    "\n",
    "# 1. Final trained model\n",
    "final_model_zip = create_downloadable_archive(\n",
    "    \"./whisper-small-ta-noisy-robust-final\", \n",
    "    \"whisper-small-ta-noisy-robust-final\"\n",
    ")\n",
    "\n",
    "# 2. Last checkpoint from training\n",
    "checkpoint_dir = \"./whisper-small-ta-noisy-robust\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    # Find the last checkpoint\n",
    "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        # Sort by checkpoint number\n",
    "        checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "        last_checkpoint = checkpoints[-1]\n",
    "        last_checkpoint_path = os.path.join(checkpoint_dir, last_checkpoint)\n",
    "        \n",
    "        print(f\"📂 Found last checkpoint: {last_checkpoint}\")\n",
    "        \n",
    "        # Create archive for last checkpoint\n",
    "        last_checkpoint_zip = create_downloadable_archive(\n",
    "            last_checkpoint_path, \n",
    "            f\"whisper-small-ta-noisy-robust-{last_checkpoint}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"❌ No checkpoints found in training directory\")\n",
    "        last_checkpoint_zip = None\n",
    "else:\n",
    "    print(\"❌ Training directory does not exist\")\n",
    "    last_checkpoint_zip = None\n",
    "\n",
    "# 3. Training logs and metrics\n",
    "if os.path.exists(\"./whisper-small-ta-noisy-robust/runs\"):\n",
    "    tensorboard_logs_zip = create_downloadable_archive(\n",
    "        \"./whisper-small-ta-noisy-robust/runs\", \n",
    "        \"whisper-small-ta-noisy-robust-tensorboard-logs\"\n",
    "    )\n",
    "else:\n",
    "    tensorboard_logs_zip = None\n",
    "\n",
    "# Create a comprehensive package with all files\n",
    "print(\"\\n📦 Creating comprehensive model package...\")\n",
    "comprehensive_package = \"whisper-small-ta-noisy-robust-complete\"\n",
    "os.makedirs(comprehensive_package, exist_ok=True)\n",
    "\n",
    "# Copy final model\n",
    "if os.path.exists(\"./whisper-small-ta-noisy-robust-final\"):\n",
    "    shutil.copytree(\"./whisper-small-ta-noisy-robust-final\", \n",
    "                    f\"{comprehensive_package}/final_model\", \n",
    "                    dirs_exist_ok=True)\n",
    "\n",
    "# Copy last checkpoint\n",
    "if os.path.exists(last_checkpoint_path):\n",
    "    shutil.copytree(last_checkpoint_path, \n",
    "                    f\"{comprehensive_package}/last_checkpoint\", \n",
    "                    dirs_exist_ok=True)\n",
    "\n",
    "# Copy training data splits\n",
    "if os.path.exists(\"train.csv\"):\n",
    "    shutil.copy(\"train.csv\", f\"{comprehensive_package}/train.csv\")\n",
    "if os.path.exists(\"val.csv\"):\n",
    "    shutil.copy(\"val.csv\", f\"{comprehensive_package}/val.csv\")\n",
    "\n",
    "# Create README file\n",
    "readme_content = f\"\"\"# Whisper small Tamil Noisy-Robust ASR Model\n",
    "\n",
    "## Training Information\n",
    "- small Model: openai/whisper-small\n",
    "- Language: Tamil (ta)\n",
    "- Task: Transcription\n",
    "- Dataset: Noisy audio data for ASR\n",
    "- Training Strategy: Noise-robust fine-tuning\n",
    "\n",
    "## Model Files\n",
    "- `final_model/`: Complete trained model ready for inference\n",
    "- `last_checkpoint/`: Last training checkpoint\n",
    "- `train.csv`: Training data split\n",
    "- `val.csv`: Validation data split\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "# Load the model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"./final_model\")\n",
    "processor = WhisperProcessor.from_pretrained(\"./final_model\")\n",
    "\n",
    "# Use for inference\n",
    "# (Add your inference code here)\n",
    "```\n",
    "\n",
    "## Training Configuration\n",
    "- Batch Size: 16 (per device)\n",
    "- Learning Rate: 1e-5\n",
    "- Epochs: 15\n",
    "- Early Stopping: 3 epochs patience\n",
    "- Optimizer: AdamW with weight decay\n",
    "\n",
    "## Evaluation Metrics\n",
    "- WER (Word Error Rate)\n",
    "- CER (Character Error Rate)\n",
    "- SER (Sentence Error Rate)\n",
    "- TER (Token Error Rate)\n",
    "\n",
    "Training completed successfully!\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{comprehensive_package}/README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "# Create comprehensive zip\n",
    "comprehensive_zip = create_downloadable_archive(\n",
    "    comprehensive_package, \n",
    "    \"whisper-small-ta-noisy-robust-complete\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T12:34:17.002022Z",
     "iopub.status.busy": "2025-07-13T12:34:17.001740Z",
     "iopub.status.idle": "2025-07-13T12:34:17.015590Z",
     "shell.execute_reply": "2025-07-13T12:34:17.014913Z",
     "shell.execute_reply.started": "2025-07-13T12:34:17.001999Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📥 DOWNLOAD LINKS FOR KAGGLE\n",
      "============================================================\n",
      "🎯 Final Model: whisper-small-ta-noisy-robust-final.zip\n",
      "📍 Last Checkpoint: whisper-small-ta-noisy-robust-checkpoint-432.zip\n",
      "📊 TensorBoard Logs: whisper-small-ta-noisy-robust-tensorboard-logs.zip\n",
      "📦 Complete Package: whisper-small-ta-noisy-robust-complete.zip\n",
      "\n",
      "============================================================\n",
      "🔗 DIRECT DOWNLOAD COMMANDS\n",
      "============================================================\n",
      "\n",
      "# Download Final Trained Model\n",
      "# File: whisper-small-ta-noisy-robust-final.zip\n",
      "# Size: 853.71 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='whisper-small-ta-noisy-robust-final.zip' target='_blank'>whisper-small-ta-noisy-robust-final.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/whisper-small-ta-noisy-robust-final.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Download Last Checkpoint\n",
      "# File: whisper-small-ta-noisy-robust-checkpoint-432.zip\n",
      "# Size: 2552.72 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='whisper-small-ta-noisy-robust-checkpoint-432.zip' target='_blank'>whisper-small-ta-noisy-robust-checkpoint-432.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/whisper-small-ta-noisy-robust-checkpoint-432.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Download TensorBoard Logs\n",
      "# File: whisper-small-ta-noisy-robust-tensorboard-logs.zip\n",
      "# Size: 0.00 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='whisper-small-ta-noisy-robust-tensorboard-logs.zip' target='_blank'>whisper-small-ta-noisy-robust-tensorboard-logs.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/whisper-small-ta-noisy-robust-tensorboard-logs.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Download Complete Package\n",
      "# File: whisper-small-ta-noisy-robust-complete.zip\n",
      "# Size: 3406.64 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='whisper-small-ta-noisy-robust-complete.zip' target='_blank'>whisper-small-ta-noisy-robust-complete.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/whisper-small-ta-noisy-robust-complete.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✅ All model files are ready for download!\n",
      "📝 Click the links above to download the files\n",
      "💡 The 'Complete Package' contains everything you need\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 13. DISPLAY DOWNLOAD LINKS\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📥 DOWNLOAD LINKS FOR KAGGLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "download_links = []\n",
    "\n",
    "if final_model_zip:\n",
    "    download_links.append((\"Final Trained Model\", final_model_zip))\n",
    "    print(f\"🎯 Final Model: {final_model_zip}\")\n",
    "\n",
    "if last_checkpoint_zip:\n",
    "    download_links.append((\"Last Checkpoint\", last_checkpoint_zip))\n",
    "    print(f\"📍 Last Checkpoint: {last_checkpoint_zip}\")\n",
    "\n",
    "if tensorboard_logs_zip:\n",
    "    download_links.append((\"TensorBoard Logs\", tensorboard_logs_zip))\n",
    "    print(f\"📊 TensorBoard Logs: {tensorboard_logs_zip}\")\n",
    "\n",
    "if comprehensive_zip:\n",
    "    download_links.append((\"Complete Package\", comprehensive_zip))\n",
    "    print(f\"📦 Complete Package: {comprehensive_zip}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔗 DIRECT DOWNLOAD COMMANDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create download commands for each file\n",
    "for name, file_path in download_links:\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        print(f\"\\n# Download {name}\")\n",
    "        print(f\"# File: {file_path}\")\n",
    "        print(f\"# Size: {os.path.getsize(file_path) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        # For Kaggle, files in the working directory are automatically available for download\n",
    "        # Just need to display the file link\n",
    "        display(FileLink(file_path))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ All model files are ready for download!\")\n",
    "print(\"📝 Click the links above to download the files\")\n",
    "print(\"💡 The 'Complete Package' contains everything you need\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7798388,
     "sourceId": 12368553,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7799096,
     "sourceId": 12369551,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7810055,
     "sourceId": 12385975,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7820165,
     "sourceId": 12400769,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
